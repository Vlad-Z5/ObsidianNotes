# Architecture Analysis Tools

## Architecture Analysis Fundamentals

Architecture analysis tools are essential for understanding, evaluating, and improving software systems. This comprehensive guide covers production-ready tools and techniques for analyzing architecture quality, dependencies, performance, and compliance.

### Static Analysis

Static analysis examines code without executing it, providing insights into structure, quality, and potential issues.

#### Code Complexity Analysis

**Definition:** Measuring code complexity to identify maintainability risks and refactoring opportunities.

**Example - Comprehensive Complexity Analyzer:**
```python
import ast
import os
import json
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from collections import defaultdict
import networkx as nx

@dataclass
class ComplexityMetrics:
    """Container for code complexity metrics"""
    cyclomatic_complexity: int
    cognitive_complexity: int
    lines_of_code: int
    halstead_volume: float
    maintainability_index: float
    nesting_depth: int
    parameter_count: int
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'cyclomatic_complexity': self.cyclomatic_complexity,
            'cognitive_complexity': self.cognitive_complexity,
            'lines_of_code': self.lines_of_code,
            'halstead_volume': self.halstead_volume,
            'maintainability_index': self.maintainability_index,
            'nesting_depth': self.nesting_depth,
            'parameter_count': self.parameter_count
        }

class ComplexityAnalyzer:
    """Comprehensive code complexity analysis tool"""
    
    def __init__(self):
        self.operators = set()
        self.operands = set()
        self.operator_count = 0
        self.operand_count = 0
    
    def analyze_file(self, file_path: str) -> Dict[str, ComplexityMetrics]:
        """Analyze complexity of all functions in a file"""
        with open(file_path, 'r', encoding='utf-8') as f:
            source_code = f.read()
        
        tree = ast.parse(source_code)
        
        results = {}
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                metrics = self._analyze_function(node, source_code)
                results[node.name] = metrics
        
        return results
    
    def _analyze_function(self, func_node: ast.FunctionDef, source_code: str) -> ComplexityMetrics:
        """Analyze complexity metrics for a single function"""
        
        # Reset counters
        self.operators.clear()
        self.operands.clear()
        self.operator_count = 0
        self.operand_count = 0
        
        # Calculate various complexity metrics
        cyclomatic = self._calculate_cyclomatic_complexity(func_node)
        cognitive = self._calculate_cognitive_complexity(func_node)
        loc = self._count_lines_of_code(func_node, source_code)
        halstead = self._calculate_halstead_volume(func_node)
        maintainability = self._calculate_maintainability_index(
            cyclomatic, loc, halstead
        )
        nesting = self._calculate_max_nesting_depth(func_node)
        params = len(func_node.args.args)
        
        return ComplexityMetrics(
            cyclomatic_complexity=cyclomatic,
            cognitive_complexity=cognitive,
            lines_of_code=loc,
            halstead_volume=halstead,
            maintainability_index=maintainability,
            nesting_depth=nesting,
            parameter_count=params
        )
    
    def _calculate_cyclomatic_complexity(self, node: ast.AST) -> int:
        """Calculate McCabe cyclomatic complexity"""
        complexity = 1  # Base complexity
        
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1
            elif isinstance(child, (ast.And, ast.Or)):
                complexity += 1
            elif isinstance(child, ast.comprehension):
                complexity += 1
        
        return complexity
    
    def _calculate_cognitive_complexity(self, node: ast.AST) -> int:
        """Calculate cognitive complexity (more aligned with human perception)"""
        complexity = 0
        nesting_level = 0
        
        def visit_node(n, level):
            nonlocal complexity, nesting_level
            
            # Increment for control structures
            if isinstance(n, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1 + level
            elif isinstance(n, ast.ExceptHandler):
                complexity += 1 + level
            elif isinstance(n, (ast.And, ast.Or)):
                complexity += 1
            elif isinstance(n, ast.BoolOp):
                complexity += len(n.values) - 1
            
            # Handle nesting
            if isinstance(n, (ast.If, ast.While, ast.For, ast.AsyncFor, ast.With, ast.Try)):
                for child in ast.iter_child_nodes(n):
                    visit_node(child, level + 1)
            else:
                for child in ast.iter_child_nodes(n):
                    visit_node(child, level)
        
        visit_node(node, 0)
        return complexity
    
    def _count_lines_of_code(self, func_node: ast.FunctionDef, source_code: str) -> int:
        """Count actual lines of code (excluding comments and blank lines)"""
        lines = source_code.split('\n')
        start_line = func_node.lineno - 1
        end_line = func_node.end_lineno if hasattr(func_node, 'end_lineno') else len(lines)
        
        loc = 0
        for i in range(start_line, min(end_line, len(lines))):
            line = lines[i].strip()
            if line and not line.startswith('#'):
                loc += 1
        
        return loc
    
    def _calculate_halstead_volume(self, node: ast.AST) -> float:
        """Calculate Halstead volume (program length Ã— log2(vocabulary))"""
        self._collect_halstead_metrics(node)
        
        vocabulary = len(self.operators) + len(self.operands)
        length = self.operator_count + self.operand_count
        
        if vocabulary <= 1:
            return 0.0
        
        import math
        return length * math.log2(vocabulary)
    
    def _collect_halstead_metrics(self, node: ast.AST):
        """Collect operators and operands for Halstead metrics"""
        for child in ast.walk(node):
            if isinstance(child, ast.BinOp):
                self.operators.add(type(child.op).__name__)
                self.operator_count += 1
            elif isinstance(child, ast.UnaryOp):
                self.operators.add(type(child.op).__name__)
                self.operator_count += 1
            elif isinstance(child, ast.Compare):
                for op in child.ops:
                    self.operators.add(type(op).__name__)
                    self.operator_count += 1
            elif isinstance(child, ast.Name):
                self.operands.add(child.id)
                self.operand_count += 1
            elif isinstance(child, ast.Constant):
                self.operands.add(str(child.value))
                self.operand_count += 1
    
    def _calculate_maintainability_index(self, cyclomatic: int, loc: int, halstead: float) -> float:
        """Calculate maintainability index (0-100, higher is better)"""
        if loc == 0:
            return 100.0
        
        import math
        
        # Simplified maintainability index calculation
        mi = max(0, (171 - 5.2 * math.log(halstead) - 0.23 * cyclomatic - 16.2 * math.log(loc)) * 100 / 171)
        return round(mi, 2)
    
    def _calculate_max_nesting_depth(self, node: ast.AST) -> int:
        """Calculate maximum nesting depth"""
        def get_depth(n, current_depth=0):
            max_depth = current_depth
            
            for child in ast.iter_child_nodes(n):
                if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor, ast.With, ast.Try)):
                    depth = get_depth(child, current_depth + 1)
                    max_depth = max(max_depth, depth)
                else:
                    depth = get_depth(child, current_depth)
                    max_depth = max(max_depth, depth)
            
            return max_depth
        
        return get_depth(node)

class ProjectComplexityAnalyzer:
    """Analyze complexity across an entire project"""
    
    def __init__(self, project_root: str):
        self.project_root = project_root
        self.analyzer = ComplexityAnalyzer()
        self.results = {}
    
    def analyze_project(self, file_patterns: List[str] = None) -> Dict[str, Any]:
        """Analyze complexity for entire project"""
        if file_patterns is None:
            file_patterns = ['**/*.py']
        
        import glob
        
        all_files = []
        for pattern in file_patterns:
            files = glob.glob(os.path.join(self.project_root, pattern), recursive=True)
            all_files.extend(files)
        
        project_metrics = {
            'files': {},
            'summary': {
                'total_files': 0,
                'total_functions': 0,
                'average_complexity': 0.0,
                'high_complexity_functions': [],
                'maintainability_concerns': []
            }
        }
        
        total_complexity = 0
        total_functions = 0
        
        for file_path in all_files:
            try:
                relative_path = os.path.relpath(file_path, self.project_root)
                file_metrics = self.analyzer.analyze_file(file_path)
                
                project_metrics['files'][relative_path] = {
                    'functions': {name: metrics.to_dict() for name, metrics in file_metrics.items()},
                    'summary': self._summarize_file_metrics(file_metrics)
                }
                
                # Accumulate project-wide statistics
                for func_name, metrics in file_metrics.items():
                    total_complexity += metrics.cyclomatic_complexity
                    total_functions += 1
                    
                    # Flag high complexity functions
                    if metrics.cyclomatic_complexity > 10:
                        project_metrics['summary']['high_complexity_functions'].append({
                            'file': relative_path,
                            'function': func_name,
                            'complexity': metrics.cyclomatic_complexity
                        })
                    
                    # Flag maintainability concerns
                    if metrics.maintainability_index < 50:
                        project_metrics['summary']['maintainability_concerns'].append({
                            'file': relative_path,
                            'function': func_name,
                            'maintainability_index': metrics.maintainability_index
                        })
                
                project_metrics['summary']['total_files'] += 1
                
            except Exception as e:
                print(f"Error analyzing {file_path}: {e}")
        
        project_metrics['summary']['total_functions'] = total_functions
        project_metrics['summary']['average_complexity'] = (
            total_complexity / total_functions if total_functions > 0 else 0
        )
        
        return project_metrics
    
    def _summarize_file_metrics(self, file_metrics: Dict[str, ComplexityMetrics]) -> Dict[str, Any]:
        """Create summary statistics for a file"""
        if not file_metrics:
            return {}
        
        complexities = [m.cyclomatic_complexity for m in file_metrics.values()]
        maintainabilities = [m.maintainability_index for m in file_metrics.values()]
        
        return {
            'function_count': len(file_metrics),
            'average_complexity': sum(complexities) / len(complexities),
            'max_complexity': max(complexities),
            'average_maintainability': sum(maintainabilities) / len(maintainabilities),
            'functions_over_complexity_threshold': len([c for c in complexities if c > 10])
        }
    
    def generate_report(self, output_file: str = None) -> str:
        """Generate human-readable complexity report"""
        if not self.results:
            self.results = self.analyze_project()
        
        report = []
        report.append("# Code Complexity Analysis Report")
        report.append(f"Project: {self.project_root}")
        report.append("")
        
        # Summary
        summary = self.results['summary']
        report.append("## Summary")
        report.append(f"- Total Files: {summary['total_files']}")
        report.append(f"- Total Functions: {summary['total_functions']}")
        report.append(f"- Average Complexity: {summary['average_complexity']:.2f}")
        report.append("")
        
        # High complexity functions
        if summary['high_complexity_functions']:
            report.append("## High Complexity Functions (Complexity > 10)")
            for func in summary['high_complexity_functions']:
                report.append(f"- {func['file']}::{func['function']} (Complexity: {func['complexity']})")
            report.append("")
        
        # Maintainability concerns
        if summary['maintainability_concerns']:
            report.append("## Maintainability Concerns (Index < 50)")
            for concern in summary['maintainability_concerns']:
                report.append(f"- {concern['file']}::{concern['function']} (Index: {concern['maintainability_index']:.2f})")
            report.append("")
        
        # File details
        report.append("## File Details")
        for file_path, file_data in self.results['files'].items():
            file_summary = file_data['summary']
            if file_summary:
                report.append(f"### {file_path}")
                report.append(f"- Functions: {file_summary['function_count']}")
                report.append(f"- Average Complexity: {file_summary['average_complexity']:.2f}")
                report.append(f"- Max Complexity: {file_summary['max_complexity']}")
                report.append(f"- Average Maintainability: {file_summary['average_maintainability']:.2f}")
                report.append("")
        
        report_text = "\n".join(report)
        
        if output_file:
            with open(output_file, 'w') as f:
                f.write(report_text)
        
        return report_text
```

#### Dependency Analysis

**Purpose:** Understanding component relationships and identifying architectural issues.

**Example - Advanced Dependency Analyzer:**
```python
import ast
import os
import re
from typing import Dict, List, Set, Tuple, Any
from dataclasses import dataclass
from collections import defaultdict, deque
import networkx as nx
import json

@dataclass
class Dependency:
    """Represents a dependency relationship"""
    source: str
    target: str
    dependency_type: str  # import, inheritance, composition, aggregation
    line_number: int
    context: str

@dataclass
class DependencyIssue:
    """Represents a dependency-related issue"""
    issue_type: str  # circular, too_many_dependencies, unstable
    severity: str  # low, medium, high, critical
    description: str
    affected_components: List[str]
    suggestions: List[str]

class DependencyAnalyzer:
    """Comprehensive dependency analysis tool"""
    
    def __init__(self, project_root: str):
        self.project_root = project_root
        self.dependencies = []
        self.modules = set()
        self.dependency_graph = nx.DiGraph()
        self.issues = []
    
    def analyze_project(self) -> Dict[str, Any]:
        """Analyze dependencies for entire project"""
        self._discover_modules()
        self._extract_dependencies()
        self._build_dependency_graph()
        self._detect_issues()
        
        return {
            'modules': list(self.modules),
            'dependencies': [self._dependency_to_dict(d) for d in self.dependencies],
            'graph_metrics': self._calculate_graph_metrics(),
            'issues': [self._issue_to_dict(i) for i in self.issues],
            'circular_dependencies': self._find_circular_dependencies(),
            'dependency_matrix': self._create_dependency_matrix(),
            'stability_metrics': self._calculate_stability_metrics()
        }
    
    def _discover_modules(self):
        """Discover all Python modules in the project"""
        for root, dirs, files in os.walk(self.project_root):
            # Skip common non-source directories
            dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['__pycache__', 'venv', 'env']]
            
            for file in files:
                if file.endswith('.py') and not file.startswith('.'):
                    file_path = os.path.join(root, file)
                    module_name = self._path_to_module_name(file_path)
                    self.modules.add(module_name)
    
    def _path_to_module_name(self, file_path: str) -> str:
        """Convert file path to Python module name"""
        rel_path = os.path.relpath(file_path, self.project_root)
        module_path = rel_path.replace(os.sep, '.').replace('.py', '')
        
        # Handle __init__.py files
        if module_path.endswith('.__init__'):
            module_path = module_path[:-9]
        
        return module_path
    
    def _extract_dependencies(self):
        """Extract dependencies from source code"""
        for root, dirs, files in os.walk(self.project_root):
            dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['__pycache__', 'venv', 'env']]
            
            for file in files:
                if file.endswith('.py'):
                    file_path = os.path.join(root, file)
                    self._analyze_file_dependencies(file_path)
    
    def _analyze_file_dependencies(self, file_path: str):
        """Analyze dependencies in a single file"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            source_module = self._path_to_module_name(file_path)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        self._add_dependency(
                            source_module, alias.name, 'import', 
                            node.lineno, f"import {alias.name}"
                        )
                
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        self._add_dependency(
                            source_module, node.module, 'import', 
                            node.lineno, f"from {node.module} import ..."
                        )
                
                elif isinstance(node, ast.ClassDef):
                    # Check for inheritance
                    for base in node.bases:
                        if isinstance(base, ast.Name):
                            self._add_dependency(
                                source_module, base.id, 'inheritance',
                                node.lineno, f"class {node.name}({base.id})"
                            )
                        elif isinstance(base, ast.Attribute):
                            # Handle module.ClassName inheritance
                            base_name = self._extract_full_name(base)
                            if base_name:
                                self._add_dependency(
                                    source_module, base_name, 'inheritance',
                                    node.lineno, f"class {node.name}({base_name})"
                                )
        
        except Exception as e:
            print(f"Error analyzing {file_path}: {e}")
    
    def _add_dependency(self, source: str, target: str, dep_type: str, line: int, context: str):
        """Add a dependency to the collection"""
        # Filter out standard library and external dependencies for internal analysis
        if not self._is_internal_module(target):
            return
        
        dependency = Dependency(
            source=source,
            target=target,
            dependency_type=dep_type,
            line_number=line,
            context=context
        )
        
        self.dependencies.append(dependency)
    
    def _is_internal_module(self, module_name: str) -> bool:
        """Check if module is part of the project (not external)"""
        # Simple heuristic: if it starts with project components or is in our module set
        project_prefixes = self._get_project_prefixes()
        return any(module_name.startswith(prefix) for prefix in project_prefixes) or module_name in self.modules
    
    def _get_project_prefixes(self) -> List[str]:
        """Get common prefixes for project modules"""
        if not self.modules:
            return []
        
        # Extract top-level package names
        prefixes = set()
        for module in self.modules:
            parts = module.split('.')
            if len(parts) > 1:
                prefixes.add(parts[0])
        
        return list(prefixes)
    
    def _extract_full_name(self, node: ast.Attribute) -> str:
        """Extract full name from ast.Attribute node"""
        if isinstance(node.value, ast.Name):
            return f"{node.value.id}.{node.attr}"
        elif isinstance(node.value, ast.Attribute):
            parent = self._extract_full_name(node.value)
            return f"{parent}.{node.attr}" if parent else None
        return None
    
    def _build_dependency_graph(self):
        """Build NetworkX graph from dependencies"""
        for dep in self.dependencies:
            self.dependency_graph.add_edge(
                dep.source, dep.target,
                type=dep.dependency_type,
                line=dep.line_number,
                context=dep.context
            )
    
    def _calculate_graph_metrics(self) -> Dict[str, Any]:
        """Calculate various graph metrics"""
        if not self.dependency_graph.nodes():
            return {}
        
        metrics = {
            'node_count': self.dependency_graph.number_of_nodes(),
            'edge_count': self.dependency_graph.number_of_edges(),
            'density': nx.density(self.dependency_graph),
            'average_clustering': nx.average_clustering(self.dependency_graph.to_undirected()),
            'strongly_connected_components': len(list(nx.strongly_connected_components(self.dependency_graph))),
            'weakly_connected_components': len(list(nx.weakly_connected_components(self.dependency_graph)))
        }
        
        # Calculate centrality measures
        try:
            metrics['betweenness_centrality'] = nx.betweenness_centrality(self.dependency_graph)
            metrics['pagerank'] = nx.pagerank(self.dependency_graph)
            metrics['in_degree_centrality'] = nx.in_degree_centrality(self.dependency_graph)
            metrics['out_degree_centrality'] = nx.out_degree_centrality(self.dependency_graph)
        except:
            pass  # Handle edge cases with empty or disconnected graphs
        
        return metrics
    
    def _find_circular_dependencies(self) -> List[List[str]]:
        """Find all circular dependencies"""
        try:
            cycles = list(nx.simple_cycles(self.dependency_graph))
            return sorted(cycles, key=len, reverse=True)  # Sort by cycle length
        except:
            return []
    
    def _detect_issues(self):
        """Detect various dependency issues"""
        self._detect_circular_dependencies()
        self._detect_excessive_dependencies()
        self._detect_unstable_dependencies()
        self._detect_god_classes()
    
    def _detect_circular_dependencies(self):
        """Detect circular dependency issues"""
        cycles = self._find_circular_dependencies()
        
        for cycle in cycles:
            severity = 'critical' if len(cycle) <= 3 else 'high'
            
            issue = DependencyIssue(
                issue_type='circular_dependency',
                severity=severity,
                description=f"Circular dependency detected: {' -> '.join(cycle + [cycle[0]])}",
                affected_components=cycle,
                suggestions=[
                    "Break the cycle by introducing an interface or abstract class",
                    "Move common functionality to a separate module",
                    "Use dependency injection to invert the dependency"
                ]
            )
            self.issues.append(issue)
    
    def _detect_excessive_dependencies(self):
        """Detect modules with too many dependencies"""
        threshold = 20  # Configurable threshold
        
        for node in self.dependency_graph.nodes():
            out_degree = self.dependency_graph.out_degree(node)
            
            if out_degree > threshold:
                issue = DependencyIssue(
                    issue_type='excessive_dependencies',
                    severity='medium',
                    description=f"Module '{node}' has {out_degree} outgoing dependencies (threshold: {threshold})",
                    affected_components=[node],
                    suggestions=[
                        "Split the module into smaller, more focused modules",
                        "Use composition instead of inheritance",
                        "Introduce facades or adapters to reduce direct dependencies"
                    ]
                )
                self.issues.append(issue)
    
    def _detect_unstable_dependencies(self):
        """Detect unstable dependency patterns"""
        stability_metrics = self._calculate_stability_metrics()
        
        for module, metrics in stability_metrics.items():
            if metrics['instability'] > 0.8 and metrics['abstractness'] < 0.2:
                issue = DependencyIssue(
                    issue_type='unstable_dependency',
                    severity='medium',
                    description=f"Module '{module}' is unstable (I={metrics['instability']:.2f}) and concrete (A={metrics['abstractness']:.2f})",
                    affected_components=[module],
                    suggestions=[
                        "Make the module more abstract by introducing interfaces",
                        "Reduce incoming dependencies",
                        "Split concrete implementation from abstract interface"
                    ]
                )
                self.issues.append(issue)
    
    def _detect_god_classes(self):
        """Detect modules that are depended upon by too many others"""
        threshold = 15  # Configurable threshold
        
        for node in self.dependency_graph.nodes():
            in_degree = self.dependency_graph.in_degree(node)
            
            if in_degree > threshold:
                issue = DependencyIssue(
                    issue_type='god_class',
                    severity='high',
                    description=f"Module '{node}' is depended upon by {in_degree} other modules (threshold: {threshold})",
                    affected_components=[node],
                    suggestions=[
                        "Split the module into smaller, more cohesive modules",
                        "Extract interfaces to reduce coupling",
                        "Use the Single Responsibility Principle to guide refactoring"
                    ]
                )
                self.issues.append(issue)
    
    def _calculate_stability_metrics(self) -> Dict[str, Dict[str, float]]:
        """Calculate Martin's stability metrics (Instability and Abstractness)"""
        metrics = {}
        
        for node in self.dependency_graph.nodes():
            afferent = self.dependency_graph.in_degree(node)  # Ca (incoming)
            efferent = self.dependency_graph.out_degree(node)  # Ce (outgoing)
            
            # Instability: I = Ce / (Ca + Ce)
            instability = efferent / (afferent + efferent) if (afferent + efferent) > 0 else 0
            
            # Abstractness: A = Abstract classes / Total classes (simplified to 0 for now)
            abstractness = 0  # Would need AST analysis to determine actual abstractness
            
            # Distance from main sequence: D = |A + I - 1|
            distance = abs(abstractness + instability - 1)
            
            metrics[node] = {
                'afferent_coupling': afferent,
                'efferent_coupling': efferent,
                'instability': instability,
                'abstractness': abstractness,
                'distance_from_main_sequence': distance
            }
        
        return metrics
    
    def _create_dependency_matrix(self) -> Dict[str, Any]:
        """Create dependency structure matrix (DSM)"""
        nodes = sorted(self.dependency_graph.nodes())
        matrix = [[0 for _ in nodes] for _ in nodes]
        
        node_to_index = {node: i for i, node in enumerate(nodes)}
        
        for edge in self.dependency_graph.edges():
            source_idx = node_to_index[edge[0]]
            target_idx = node_to_index[edge[1]]
            matrix[source_idx][target_idx] = 1
        
        return {
            'nodes': nodes,
            'matrix': matrix,
            'node_to_index': node_to_index
        }
    
    def _dependency_to_dict(self, dep: Dependency) -> Dict[str, Any]:
        """Convert Dependency to dictionary"""
        return {
            'source': dep.source,
            'target': dep.target,
            'type': dep.dependency_type,
            'line': dep.line_number,
            'context': dep.context
        }
    
    def _issue_to_dict(self, issue: DependencyIssue) -> Dict[str, Any]:
        """Convert DependencyIssue to dictionary"""
        return {
            'type': issue.issue_type,
            'severity': issue.severity,
            'description': issue.description,
            'affected_components': issue.affected_components,
            'suggestions': issue.suggestions
        }
    
    def export_graph(self, output_file: str, format: str = 'gml'):
        """Export dependency graph to file"""
        if format == 'gml':
            nx.write_gml(self.dependency_graph, output_file)
        elif format == 'graphml':
            nx.write_graphml(self.dependency_graph, output_file)
        elif format == 'dot':
            nx.drawing.nx_pydot.write_dot(self.dependency_graph, output_file)
        else:
            raise ValueError(f"Unsupported format: {format}")
    
    def generate_report(self, output_file: str = None) -> str:
        """Generate comprehensive dependency analysis report"""
        analysis_results = self.analyze_project()
        
        report = []
        report.append("# Dependency Analysis Report")
        report.append("")
        
        # Summary
        metrics = analysis_results['graph_metrics']
        report.append("## Summary")
        report.append(f"- Total Modules: {metrics.get('node_count', 0)}")
        report.append(f"- Total Dependencies: {metrics.get('edge_count', 0)}")
        report.append(f"- Graph Density: {metrics.get('density', 0):.3f}")
        report.append(f"- Average Clustering: {metrics.get('average_clustering', 0):.3f}")
        report.append("")
        
        # Issues
        issues = analysis_results['issues']
        if issues:
            report.append("## Issues Found")
            issue_by_severity = defaultdict(list)
            for issue in issues:
                issue_by_severity[issue['severity']].append(issue)
            
            for severity in ['critical', 'high', 'medium', 'low']:
                if severity in issue_by_severity:
                    report.append(f"### {severity.title()} Issues")
                    for issue in issue_by_severity[severity]:
                        report.append(f"- **{issue['type']}**: {issue['description']}")
                        for suggestion in issue['suggestions'][:2]:  # Limit suggestions
                            report.append(f"  - {suggestion}")
                    report.append("")
        
        # Circular Dependencies
        cycles = analysis_results['circular_dependencies']
        if cycles:
            report.append("## Circular Dependencies")
            for i, cycle in enumerate(cycles[:5], 1):  # Show top 5
                cycle_str = " -> ".join(cycle + [cycle[0]])
                report.append(f"{i}. {cycle_str}")
            report.append("")
        
        # Top Dependencies
        if 'out_degree_centrality' in metrics:
            report.append("## Most Dependent Modules")
            out_centrality = metrics['out_degree_centrality']
            top_dependent = sorted(out_centrality.items(), key=lambda x: x[1], reverse=True)[:10]
            for module, centrality in top_dependent:
                report.append(f"- {module}: {centrality:.3f}")
            report.append("")
        
        report_text = "\n".join(report)
        
        if output_file:
            with open(output_file, 'w') as f:
                f.write(report_text)
        
        return report_text

# Usage Example
def analyze_project_dependencies(project_path: str):
    """Complete dependency analysis workflow"""
    analyzer = DependencyAnalyzer(project_path)
    
    # Perform analysis
    results = analyzer.analyze_project()
    
    # Generate reports
    report = analyzer.generate_report('dependency_report.md')
    
    # Export graph for visualization
    analyzer.export_graph('dependencies.gml', 'gml')
    
    # Save detailed results
    with open('dependency_analysis.json', 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"Analysis complete. Found {len(results['issues'])} issues.")
    print(f"Report saved to dependency_report.md")
    
    return results
```

### Dynamic Analysis

Dynamic analysis examines software behavior during execution, providing insights into runtime performance, resource usage, and actual behavior.

#### Performance Profiling

**Purpose:** Understanding runtime performance characteristics and identifying bottlenecks.

**Example - Comprehensive Performance Profiler:**
```python
import cProfile
import pstats
import time
import psutil
import threading
import functools
from typing import Dict, List, Any, Callable, Optional
from dataclasses import dataclass, field
from collections import defaultdict
import json
import sys
import tracemalloc
import gc
import linecache

@dataclass
class PerformanceMetrics:
    """Container for performance measurement results"""
    execution_time: float
    cpu_usage: float
    memory_usage: float
    memory_peak: float
    function_calls: int
    io_operations: int
    gc_collections: int
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'execution_time': self.execution_time,
            'cpu_usage': self.cpu_usage,
            'memory_usage': self.memory_usage,
            'memory_peak': self.memory_peak,
            'function_calls': self.function_calls,
            'io_operations': self.io_operations,
            'gc_collections': self.gc_collections
        }

@dataclass
class HotSpot:
    """Represents a performance hotspot"""
    function_name: str
    file_path: str
    line_number: int
    cumulative_time: float
    own_time: float
    call_count: int
    time_per_call: float
    percentage_total: float

class PerformanceProfiler:
    """Comprehensive performance profiling tool"""
    
    def __init__(self):
        self.profile_data = {}
        self.memory_snapshots = []
        self.cpu_usage_history = []
        self.is_monitoring = False
        self.monitor_thread = None
        
    def profile_function(self, func: Callable) -> Callable:
        """Decorator to profile individual functions"""
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            return self._profile_execution(func, *args, **kwargs)
        return wrapper
    
    def _profile_execution(self, func: Callable, *args, **kwargs):
        """Profile a single function execution"""
        # Start performance monitoring
        start_time = time.perf_counter()
        process = psutil.Process()
        
        # Memory tracking
        tracemalloc.start()
        gc_before = len(gc.get_objects())
        
        # CPU profiling
        profiler = cProfile.Profile()
        profiler.enable()
        
        try:
            # Execute function
            result = func(*args, **kwargs)
            
        finally:
            # Stop profiling
            profiler.disable()
            end_time = time.perf_counter()
            
            # Get memory info
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            gc_after = len(gc.get_objects())
            
            # Calculate metrics
            execution_time = end_time - start_time
            cpu_percent = process.cpu_percent()
            memory_info = process.memory_info()
            
            metrics = PerformanceMetrics(
                execution_time=execution_time,
                cpu_usage=cpu_percent,
                memory_usage=current / 1024 / 1024,  # MB
                memory_peak=peak / 1024 / 1024,  # MB
                function_calls=self._count_function_calls(profiler),
                io_operations=0,  # Could be enhanced to track I/O
                gc_collections=gc_after - gc_before
            )
            
            # Store profile data
            self.profile_data[func.__name__] = {
                'metrics': metrics,
                'profiler_stats': profiler,
                'timestamp': time.time()
            }
            
        return result
    
    def _count_function_calls(self, profiler: cProfile.Profile) -> int:
        """Count total function calls from profiler"""
        stats = pstats.Stats(profiler)
        return sum(stat[0] for stat in stats.stats.values())
    
    def start_system_monitoring(self, interval: float = 1.0):
        """Start continuous system monitoring"""
        self.is_monitoring = True
        self.cpu_usage_history = []
        self.memory_snapshots = []
        
        def monitor():
            process = psutil.Process()
            while self.is_monitoring:
                # CPU usage
                cpu_percent = process.cpu_percent()
                self.cpu_usage_history.append({
                    'timestamp': time.time(),
                    'cpu_percent': cpu_percent
                })
                
                # Memory usage
                memory_info = process.memory_info()
                self.memory_snapshots.append({
                    'timestamp': time.time(),
                    'rss': memory_info.rss / 1024 / 1024,  # MB
                    'vms': memory_info.vms / 1024 / 1024   # MB
                })
                
                time.sleep(interval)
        
        self.monitor_thread = threading.Thread(target=monitor, daemon=True)
        self.monitor_thread.start()
    
    def stop_system_monitoring(self):
        """Stop continuous system monitoring"""
        self.is_monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=2.0)
    
    def profile_code_block(self, code_block: str, globals_dict: Dict = None, locals_dict: Dict = None) -> Dict[str, Any]:
        """Profile a block of code"""
        if globals_dict is None:
            globals_dict = {}
        if locals_dict is None:
            locals_dict = {}
        
        # Setup profiling
        profiler = cProfile.Profile()
        tracemalloc.start()
        start_time = time.perf_counter()
        
        try:
            profiler.runcodeex(compile(code_block, '<string>', 'exec'), globals_dict, locals_dict)
        finally:
            end_time = time.perf_counter()
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()
        
        # Analyze results
        stats = pstats.Stats(profiler)
        hotspots = self._extract_hotspots(stats)
        
        return {
            'execution_time': end_time - start_time,
            'memory_current': current / 1024 / 1024,
            'memory_peak': peak / 1024 / 1024,
            'hotspots': hotspots,
            'stats': stats
        }
    
    def _extract_hotspots(self, stats: pstats.Stats, limit: int = 10) -> List[HotSpot]:
        """Extract performance hotspots from profiling stats"""
        hotspots = []
        
        stats.sort_stats('cumulative')
        total_time = stats.total_tt
        
        for i, (func_key, func_stats) in enumerate(stats.stats.items()):
            if i >= limit:
                break
            
            filename, line_number, function_name = func_key
            call_count, _, own_time, cumulative_time, callers = func_stats
            
            hotspot = HotSpot(
                function_name=function_name,
                file_path=filename,
                line_number=line_number,
                cumulative_time=cumulative_time,
                own_time=own_time,
                call_count=call_count,
                time_per_call=cumulative_time / call_count if call_count > 0 else 0,
                percentage_total=(cumulative_time / total_time * 100) if total_time > 0 else 0
            )
            
            hotspots.append(hotspot)
        
        return hotspots
    
    def analyze_memory_leaks(self) -> Dict[str, Any]:
        """Analyze potential memory leaks"""
        if not self.memory_snapshots:
            return {'error': 'No memory snapshots available'}
        
        # Calculate memory growth rate
        if len(self.memory_snapshots) < 2:
            return {'error': 'Insufficient memory snapshots'}
        
        start_memory = self.memory_snapshots[0]['rss']
        end_memory = self.memory_snapshots[-1]['rss']
        time_span = self.memory_snapshots[-1]['timestamp'] - self.memory_snapshots[0]['timestamp']
        
        growth_rate = (end_memory - start_memory) / time_span if time_span > 0 else 0
        
        # Detect memory spikes
        max_memory = max(snapshot['rss'] for snapshot in self.memory_snapshots)
        min_memory = min(snapshot['rss'] for snapshot in self.memory_snapshots)
        memory_variance = max_memory - min_memory
        
        # Simple leak detection heuristic
        potential_leak = growth_rate > 1.0  # More than 1MB/second growth
        
        return {
            'start_memory_mb': start_memory,
            'end_memory_mb': end_memory,
            'growth_rate_mb_per_sec': growth_rate,
            'max_memory_mb': max_memory,
            'memory_variance_mb': memory_variance,
            'potential_leak': potential_leak,
            'analysis_duration_sec': time_span
        }
    
    def benchmark_function(self, func: Callable, iterations: int = 100, *args, **kwargs) -> Dict[str, Any]:
        """Benchmark a function over multiple iterations"""
        execution_times = []
        memory_usages = []
        
        for _ in range(iterations):
            # Memory before
            tracemalloc.start()
            
            # Time execution
            start_time = time.perf_counter()
            func(*args, **kwargs)
            end_time = time.perf_counter()
            
            # Memory after
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            
            execution_times.append(end_time - start_time)
            memory_usages.append(current / 1024 / 1024)  # MB
        
        # Calculate statistics
        avg_time = sum(execution_times) / len(execution_times)
        min_time = min(execution_times)
        max_time = max(execution_times)
        avg_memory = sum(memory_usages) / len(memory_usages)
        
        # Calculate percentiles
        sorted_times = sorted(execution_times)
        p50_index = len(sorted_times) // 2
        p95_index = int(len(sorted_times) * 0.95)
        p99_index = int(len(sorted_times) * 0.99)
        
        return {
            'iterations': iterations,
            'average_time': avg_time,
            'min_time': min_time,
            'max_time': max_time,
            'p50_time': sorted_times[p50_index],
            'p95_time': sorted_times[p95_index] if p95_index < len(sorted_times) else sorted_times[-1],
            'p99_time': sorted_times[p99_index] if p99_index < len(sorted_times) else sorted_times[-1],
            'average_memory_mb': avg_memory,
            'operations_per_second': iterations / sum(execution_times) if sum(execution_times) > 0 else 0
        }
    
    def generate_performance_report(self, output_file: str = None) -> str:
        """Generate comprehensive performance report"""
        report = []
        report.append("# Performance Analysis Report")
        report.append("")
        
        # Function-level analysis
        if self.profile_data:
            report.append("## Function Performance Analysis")
            for func_name, data in self.profile_data.items():
                metrics = data['metrics']
                report.append(f"### {func_name}")
                report.append(f"- Execution Time: {metrics.execution_time:.4f}s")
                report.append(f"- Memory Usage: {metrics.memory_usage:.2f}MB")
                report.append(f"- Memory Peak: {metrics.memory_peak:.2f}MB")
                report.append(f"- Function Calls: {metrics.function_calls}")
                report.append("")
        
        # System monitoring analysis
        if self.cpu_usage_history:
            avg_cpu = sum(entry['cpu_percent'] for entry in self.cpu_usage_history) / len(self.cpu_usage_history)
            max_cpu = max(entry['cpu_percent'] for entry in self.cpu_usage_history)
            
            report.append("## System Resource Usage")
            report.append(f"- Average CPU Usage: {avg_cpu:.2f}%")
            report.append(f"- Peak CPU Usage: {max_cpu:.2f}%")
            report.append("")
        
        if self.memory_snapshots:
            avg_memory = sum(snapshot['rss'] for snapshot in self.memory_snapshots) / len(self.memory_snapshots)
            max_memory = max(snapshot['rss'] for snapshot in self.memory_snapshots)
            
            report.append("## Memory Usage Analysis")
            report.append(f"- Average Memory: {avg_memory:.2f}MB")
            report.append(f"- Peak Memory: {max_memory:.2f}MB")
            
            # Memory leak analysis
            leak_analysis = self.analyze_memory_leaks()
            if 'growth_rate_mb_per_sec' in leak_analysis:
                report.append(f"- Memory Growth Rate: {leak_analysis['growth_rate_mb_per_sec']:.4f}MB/s")
                report.append(f"- Potential Memory Leak: {'Yes' if leak_analysis['potential_leak'] else 'No'}")
            report.append("")
        
        # Recommendations
        report.append("## Optimization Recommendations")
        if self.profile_data:
            for func_name, data in self.profile_data.items():
                metrics = data['metrics']
                if metrics.execution_time > 1.0:
                    report.append(f"- {func_name}: Consider optimizing - execution time is {metrics.execution_time:.2f}s")
                if metrics.memory_peak > 100:
                    report.append(f"- {func_name}: High memory usage - peak {metrics.memory_peak:.2f}MB")
        
        report_text = "\n".join(report)
        
        if output_file:
            with open(output_file, 'w') as f:
                f.write(report_text)
        
        return report_text
    
    def export_profile_data(self, output_file: str):
        """Export profile data to JSON file"""
        export_data = {}
        
        # Function profiles
        export_data['function_profiles'] = {}
        for func_name, data in self.profile_data.items():
            export_data['function_profiles'][func_name] = {
                'metrics': data['metrics'].to_dict(),
                'timestamp': data['timestamp']
            }
        
        # System monitoring data
        export_data['cpu_usage_history'] = self.cpu_usage_history
        export_data['memory_snapshots'] = self.memory_snapshots
        
        # Memory leak analysis
        if self.memory_snapshots:
            export_data['memory_leak_analysis'] = self.analyze_memory_leaks()
        
        with open(output_file, 'w') as f:
            json.dump(export_data, f, indent=2, default=str)

# Usage Examples
def example_performance_profiling():
    """Example usage of performance profiling"""
    profiler = PerformanceProfiler()
    
    # Profile individual function
    @profiler.profile_function
    def expensive_operation():
        # Simulate expensive computation
        total = 0
        for i in range(1000000):
            total += i * i
        return total
    
    # Start system monitoring
    profiler.start_system_monitoring(interval=0.5)
    
    # Run the expensive operation
    result = expensive_operation()
    
    # Benchmark the function
    benchmark_results = profiler.benchmark_function(expensive_operation, iterations=10)
    
    # Stop monitoring
    profiler.stop_system_monitoring()
    
    # Generate report
    report = profiler.generate_performance_report('performance_report.md')
    profiler.export_profile_data('performance_data.json')
    
    print("Performance analysis complete!")
    print(f"Average execution time: {benchmark_results['average_time']:.4f}s")
    print(f"Operations per second: {benchmark_results['operations_per_second']:.2f}")
    
    return profiler

# Advanced Load Testing
class LoadTester:
    """Load testing framework for performance analysis"""
    
    def __init__(self):
        self.results = []
        self.is_running = False
    
    def run_load_test(self, target_function: Callable, concurrent_users: int, 
                     duration_seconds: int, ramp_up_time: int = 0) -> Dict[str, Any]:
        """Run load test with specified parameters"""
        import concurrent.futures
        import threading
        
        self.is_running = True
        start_time = time.time()
        end_time = start_time + duration_seconds
        
        # Results collection
        execution_times = []
        errors = []
        successful_requests = 0
        failed_requests = 0
        
        def worker():
            nonlocal successful_requests, failed_requests
            
            while self.is_running and time.time() < end_time:
                try:
                    request_start = time.perf_counter()
                    target_function()
                    request_end = time.perf_counter()
                    
                    execution_times.append(request_end - request_start)
                    successful_requests += 1
                    
                except Exception as e:
                    errors.append(str(e))
                    failed_requests += 1
                
                # Small delay to prevent overwhelming
                time.sleep(0.01)
        
        # Start workers
        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            # Ramp up users gradually
            futures = []
            for i in range(concurrent_users):
                if ramp_up_time > 0:
                    time.sleep(ramp_up_time / concurrent_users)
                future = executor.submit(worker)
                futures.append(future)
            
            # Wait for test duration
            time.sleep(duration_seconds)
            self.is_running = False
            
            # Wait for all workers to complete
            concurrent.futures.wait(futures, timeout=5.0)
        
        # Calculate statistics
        total_requests = successful_requests + failed_requests
        success_rate = (successful_requests / total_requests * 100) if total_requests > 0 else 0
        
        if execution_times:
            avg_response_time = sum(execution_times) / len(execution_times)
            min_response_time = min(execution_times)
            max_response_time = max(execution_times)
            
            # Calculate percentiles
            sorted_times = sorted(execution_times)
            p50 = sorted_times[len(sorted_times) // 2]
            p95 = sorted_times[int(len(sorted_times) * 0.95)]
            p99 = sorted_times[int(len(sorted_times) * 0.99)]
            
            requests_per_second = total_requests / duration_seconds
        else:
            avg_response_time = min_response_time = max_response_time = 0
            p50 = p95 = p99 = 0
            requests_per_second = 0
        
        results = {
            'test_duration': duration_seconds,
            'concurrent_users': concurrent_users,
            'total_requests': total_requests,
            'successful_requests': successful_requests,
            'failed_requests': failed_requests,
            'success_rate': success_rate,
            'requests_per_second': requests_per_second,
            'average_response_time': avg_response_time,
            'min_response_time': min_response_time,
            'max_response_time': max_response_time,
            'p50_response_time': p50,
            'p95_response_time': p95,
            'p99_response_time': p99,
            'errors': errors[:10]  # Limit error samples
        }
        
        self.results.append(results)
        return results
    
    def generate_load_test_report(self, output_file: str = None) -> str:
        """Generate load test report"""
        report = []
        report.append("# Load Test Report")
        report.append("")
        
        for i, result in enumerate(self.results, 1):
            report.append(f"## Test Run {i}")
            report.append(f"- Duration: {result['test_duration']}s")
            report.append(f"- Concurrent Users: {result['concurrent_users']}")
            report.append(f"- Total Requests: {result['total_requests']}")
            report.append(f"- Success Rate: {result['success_rate']:.2f}%")
            report.append(f"- Requests/Second: {result['requests_per_second']:.2f}")
            report.append(f"- Average Response Time: {result['average_response_time']:.4f}s")
            report.append(f"- P95 Response Time: {result['p95_response_time']:.4f}s")
            report.append(f"- P99 Response Time: {result['p99_response_time']:.4f}s")
            
            if result['errors']:
                report.append(f"- Sample Errors: {len(result['errors'])}")
                for error in result['errors'][:3]:
                    report.append(f"  - {error}")
            
            report.append("")
        
        report_text = "\n".join(report)
        
        if output_file:
            with open(output_file, 'w') as f:
                f.write(report_text)
        
        return report_text
```

This comprehensive architecture analysis tools guide provides production-ready implementations for static analysis (complexity and dependency analysis), security scanning, dynamic analysis (performance profiling), and load testing. Each tool includes detailed metrics collection, issue detection, reporting capabilities, and practical usage examples for integration into development workflows.

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"id": "todo_1", "content": "Analyze all Architecture files to identify which ones need comprehensive content like the well-populated examples", "status": "completed", "priority": "high"}, {"id": "todo_2", "content": "Read and analyze remaining Architecture files to determine content gaps", "status": "completed", "priority": "high"}, {"id": "todo_3", "content": "Populate Architecture Analysis Tools.md with comprehensive content similar to the detailed examples", "status": "completed", "priority": "high"}]